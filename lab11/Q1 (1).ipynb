{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e3ed9922-2e2d-4036-8edf-ab4b7d7f3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                              \n",
    "import pandas as pd                                     \n",
    "import random    \n",
    "import re\n",
    "import string                                                                        \n",
    "# from sklearn.datasets import fetch_20moviegroups         \n",
    "from sklearn.svm import SVC                          \n",
    "from sklearn.model_selection import GridSearchCV        \n",
    "from sklearn.metrics import accuracy_score         \n",
    "from nltk.corpus import stopwords                        \n",
    "from nltk.tokenize import word_tokenize                  \n",
    "import itertools                                         \n",
    "import warnings                                          \n",
    "warnings.filterwarnings('ignore')\n",
    "random_seed = 3116"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f9b1b8",
   "metadata": {},
   "source": [
    "<h2>1 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ca2fc5",
   "metadata": {},
   "source": [
    "<h3>In this part of the assignment, you will be using the processed dataset from previous Lab Question 1. Using the BOW and Tf-Idf representation, implement a Naive-Bayes classifier for the data. Use Laplace smoothing for the implementation. Compare your implemenation with the sklearn implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "b90ea04e-a52b-4b57-a240-81f3dfa2fb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the training data\n",
    "imdb_data=pd.read_csv('imdb_dataset.csv')\n",
    "imdb_data = imdb_data.iloc[:1000, :] # Taking a subset of data due to high computational requirement of this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ef178b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to split the dataset into train, test and validation sets\n",
    "def split_dataset(movie_vectors, targets, train_ratio, validation_ratio):\n",
    "    combined = list(zip(movie_vectors, targets))    \n",
    "    random.shuffle(combined)\n",
    "    train_rows = int(len(combined) * train_ratio)\n",
    "    validation_rows = int(len(combined) * validation_ratio)\n",
    "    X , y = list(zip(*combined))\n",
    "    X, y = list(X), list(y)\n",
    "    X_train, X_val, X_test = X[:train_rows], X[train_rows:train_rows+validation_rows], X[train_rows+validation_rows:]\n",
    "    y_train, y_val, y_test = y[:train_rows], y[train_rows:train_rows+validation_rows], y[train_rows+validation_rows:]\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3c176a73-be1b-4007-9ad9-f8165d45bbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing textual data to remove punctuation, stop-words\n",
    "# Function to Preprocess the data by removing Stopwords, punctuations and tokenizing the data\n",
    "def preprocess(movie_string):\n",
    "    # Extracting the English stopwords and converting it into a set\n",
    "    english_stop_words = set(stopwords.words('english'))\n",
    "    # Making the data into the lower case string and then tokenizing the data into word list\n",
    "    movie_string = word_tokenize(movie_string.lower())\n",
    "    # Removing stopwords and punctuations from the word list\n",
    "    movie_string = [word for word in movie_string if word not in english_stop_words and word.isalpha()]\n",
    "    # Returning the final processed data list\n",
    "    return movie_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2ba20fe3-51b8-430c-b6c3-97d9529e9ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting movie and its Target label\n",
    "movie_items = imdb_data['review']\n",
    "movie_target = imdb_data['sentiment']\n",
    "# Initializing an empty list to store processed movie items\n",
    "processed_movie = []\n",
    "# Applying Preprocessing step on all the movie items\n",
    "# Iterating for each movie items\n",
    "for n_item in movie_items:\n",
    "    #Applying preprocessing on current movie item\n",
    "    processed_movie.append(preprocess(n_item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "84b211a5-db75-457e-837a-f4651a25685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag-of-words feature representation for each text sample\n",
    "def update_word_freq(data, freq_dict): # Word freq Dictionary from the Provided data\n",
    "    # Using list comprehension to update word freq in the dictionary\n",
    "    freq_dict = {word: freq_dict.get(word, 0) + 1 for word in data}\n",
    "    return freq_dict\n",
    "\n",
    "# Create a Binary vector for a data based on Bag of Word Representation\n",
    "def bag_of_words(data, freq_dict):\n",
    "    #Initializing a vector with zeros having the length equal to total unique words in the corpus\n",
    "    sent_vector = np.zeros(shape=(len(freq_dict.keys()),))\n",
    "    for word in data:\n",
    "        if word in freq_dict.keys():\n",
    "            sent_vector[list(freq_dict.keys()).index(word)] = 1\n",
    "    return sent_vector\n",
    "\n",
    "features = 1500 # Number of features for processing\n",
    "# Creating a dictionary to store all unique words and there count in the entire corpus\n",
    "corpus_word_freq = {}\n",
    "for doc in processed_movie:\n",
    "    corpus_word_freq = update_word_freq(doc, corpus_word_freq)\n",
    "corpus_word_freq = dict(sorted(corpus_word_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "corpus_word_freq = dict(itertools.islice(corpus_word_freq.items(), features))\n",
    "\n",
    "movie_bog_vectors = []\n",
    "for doc in processed_movie:\n",
    "    # Creating a bag of word representation vector and appending it into the final list\n",
    "    movie_bog_vectors.append(bag_of_words(doc, corpus_word_freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "62cf8c35-b296-4eaa-994b-0727dd7ad6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>br</th>\n",
       "      <th>see</th>\n",
       "      <th>even</th>\n",
       "      <th>like</th>\n",
       "      <th>appear</th>\n",
       "      <th>film</th>\n",
       "      <th>go</th>\n",
       "      <th>avoid</th>\n",
       "      <th>minutes</th>\n",
       "      <th>...</th>\n",
       "      <th>noises</th>\n",
       "      <th>appropriate</th>\n",
       "      <th>masking</th>\n",
       "      <th>noise</th>\n",
       "      <th>old</th>\n",
       "      <th>israeli</th>\n",
       "      <th>russian</th>\n",
       "      <th>planes</th>\n",
       "      <th>used</th>\n",
       "      <th>us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   make   br  see  even  like  appear  film   go  avoid  minutes  ...  noises  \\\n",
       "0   0.0  1.0  0.0   0.0   0.0     0.0   0.0  1.0    0.0      0.0  ...     0.0   \n",
       "1   0.0  1.0  1.0   0.0   0.0     0.0   0.0  0.0    0.0      0.0  ...     0.0   \n",
       "2   0.0  1.0  1.0   1.0   0.0     0.0   0.0  1.0    0.0      0.0  ...     0.0   \n",
       "3   1.0  1.0  1.0   0.0   1.0     0.0   1.0  0.0    0.0      0.0  ...     0.0   \n",
       "4   1.0  1.0  1.0   0.0   0.0     0.0   1.0  0.0    0.0      0.0  ...     0.0   \n",
       "\n",
       "   appropriate  masking  noise  old  israeli  russian  planes  used   us  \n",
       "0          0.0      0.0    0.0  0.0      0.0      0.0     0.0   0.0  0.0  \n",
       "1          0.0      0.0    0.0  0.0      0.0      0.0     0.0   0.0  0.0  \n",
       "2          0.0      0.0    0.0  0.0      0.0      0.0     0.0   0.0  1.0  \n",
       "3          0.0      0.0    0.0  0.0      0.0      0.0     0.0   0.0  0.0  \n",
       "4          0.0      0.0    0.0  0.0      0.0      0.0     0.0   0.0  1.0  \n",
       "\n",
       "[5 rows x 219 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bag of Word Representation\n",
    "movie_bog_df = pd.DataFrame(movie_bog_vectors, columns=corpus_word_freq.keys())\n",
    "movie_bog_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e0a02941-a542-48fb-a5a9-3f742e85ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Term freq (TF) for a word in a data\n",
    "def term_freq(data, word):\n",
    "    return data[word]/sum(data.values())\n",
    "\n",
    "# Function to calculate the Inverse data freq (IDF) for a word in the entire Corpus\n",
    "def data_freq_inv(total_doc_freq, word, total_datas):\n",
    "    return np.log(total_datas/total_doc_freq[word] + 1)\n",
    "\n",
    "# Function to calculate the TF-IDF of a all the words individually in the entire corpus\n",
    "def tf_idf(data, total_doc_freq, total_datas):\n",
    "    data_vector = np.zeros(shape=(len(total_doc_freq.keys()),))\n",
    "    for word in data.keys():\n",
    "        if word in total_doc_freq.keys():\n",
    "            tf_idf = term_freq(data, word) * data_freq_inv(total_doc_freq, word, total_datas)\n",
    "            data_vector[list(total_doc_freq.keys()).index(word)] = tf_idf\n",
    "    return data_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "25185765-4714-4902-aa19-8720d937e450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting each data in the dataset into a TF-IDF Representation\n",
    "movie_tfidf_vectors = []\n",
    "#Iterating for each datas to generate word freq dictionary and adding tfidf vectors to movie_tfidf_vectors list\n",
    "for doc in processed_movie:\n",
    "    current_doc_dict = update_word_freq(doc, {})\n",
    "    movie_tfidf_vectors.append(tf_idf(current_doc_dict, corpus_word_freq, len(processed_movie)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8e6c09c1-830e-4d7e-885b-c6f67381ca5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>make</th>\n",
       "      <th>br</th>\n",
       "      <th>see</th>\n",
       "      <th>even</th>\n",
       "      <th>like</th>\n",
       "      <th>appear</th>\n",
       "      <th>film</th>\n",
       "      <th>go</th>\n",
       "      <th>avoid</th>\n",
       "      <th>minutes</th>\n",
       "      <th>...</th>\n",
       "      <th>noises</th>\n",
       "      <th>appropriate</th>\n",
       "      <th>masking</th>\n",
       "      <th>noise</th>\n",
       "      <th>old</th>\n",
       "      <th>israeli</th>\n",
       "      <th>russian</th>\n",
       "      <th>planes</th>\n",
       "      <th>used</th>\n",
       "      <th>us</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.043374</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.079618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.075482</td>\n",
       "      <td>0.075482</td>\n",
       "      <td>0.075482</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.106066</td>\n",
       "      <td>0.116243</td>\n",
       "      <td>0.116243</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.124332</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.124332</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.054673</td>\n",
       "      <td>0.059919</td>\n",
       "      <td>0.059919</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.071224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 219 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       make        br       see      even      like  appear      film  \\\n",
       "0  0.000000  0.043374  0.000000  0.000000  0.000000     0.0  0.000000   \n",
       "1  0.000000  0.079618  0.079618  0.000000  0.000000     0.0  0.000000   \n",
       "2  0.000000  0.075482  0.075482  0.075482  0.000000     0.0  0.000000   \n",
       "3  0.106066  0.116243  0.116243  0.000000  0.124332     0.0  0.124332   \n",
       "4  0.054673  0.059919  0.059919  0.000000  0.000000     0.0  0.064089   \n",
       "\n",
       "         go  avoid  minutes  ...  noises  appropriate  masking  noise  old  \\\n",
       "0  0.046393    0.0      0.0  ...     0.0          0.0      0.0    0.0  0.0   \n",
       "1  0.000000    0.0      0.0  ...     0.0          0.0      0.0    0.0  0.0   \n",
       "2  0.080735    0.0      0.0  ...     0.0          0.0      0.0    0.0  0.0   \n",
       "3  0.000000    0.0      0.0  ...     0.0          0.0      0.0    0.0  0.0   \n",
       "4  0.000000    0.0      0.0  ...     0.0          0.0      0.0    0.0  0.0   \n",
       "\n",
       "   israeli  russian  planes  used        us  \n",
       "0      0.0      0.0     0.0   0.0  0.000000  \n",
       "1      0.0      0.0     0.0   0.0  0.000000  \n",
       "2      0.0      0.0     0.0   0.0  0.089724  \n",
       "3      0.0      0.0     0.0   0.0  0.000000  \n",
       "4      0.0      0.0     0.0   0.0  0.071224  \n",
       "\n",
       "[5 rows x 219 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Representation\n",
    "movie_tfidf = pd.DataFrame(movie_tfidf_vectors, columns=corpus_word_freq.keys())\n",
    "movie_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630baf3b-e39e-4009-89cd-354ebdde74a7",
   "metadata": {},
   "source": [
    "<h2> Naive Bayes Classifier for Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "8dedba9e-39ee-47b7-8239-3cc9d4f0ef31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self,feature_representation, dataset, target):\n",
    "        self.feature_representation = feature_representation\n",
    "        self.dataset = np.array(dataset)\n",
    "        self.target = np.array(target)\n",
    "        self.unique_target = list(set(target))\n",
    "        self.accuracy = 0\n",
    "        self.combined = np.concatenate((self.dataset, self.target.reshape(-1,1)), axis = 1)\n",
    "    \n",
    "    # Calculate the Probability for a data represented using Bag of Words\n",
    "    def get_prob_bog(self, data, target):\n",
    "        # Calculating the probability for a class itself\n",
    "        prob_class = len(self.target[np.where(self.target == target)]) / len(self.target)\n",
    "        # Initializing the prior probability for each Word in the data\n",
    "        words_prior_prob = 1\n",
    "        # Iterating over each word in the data\n",
    "        for i in range(len(data) - 1): # If word exists in the data\n",
    "            if data[i] == 1:\n",
    "                # Calculating the Prior probability of the word given the class\n",
    "                p_word_num = len(self.combined[np.where((self.combined[:,i] == 1) & (self.combined[:,-1] == target))])\n",
    "                p_word_den = len(self.combined[np.where(self.combined[:,-1] == target)])\n",
    "                words_prior_prob *= (p_word_num / p_word_den)\n",
    "        return words_prior_prob * prob_class\n",
    "    \n",
    "    # Calculate the Probability for a data represented using TF-IDF\n",
    "    def get_prob_tfidf(self, data, target):\n",
    "        prob_class = len(self.target[np.where(self.target == target)]) / len(self.target)\n",
    "        words_prior_prob = 1\n",
    "        for i in range(len(data) - 1):\n",
    "            if data[i] == 1:\n",
    "                p_word_num = sum(self.combined[np.where((self.combined[:,i] == 1) & (self.combined[:,-1] == target))])\n",
    "                p_word_den = sum(self.combined[np.where(self.combined[:,-1] == target)])\n",
    "                words_prior_prob *= (p_word_num / p_word_den)\n",
    "        return words_prior_prob * prob_class\n",
    "\n",
    "    # Predictions on the dataset provided and calculate the Accuracy\n",
    "    def predict(self):\n",
    "        for index, doc in enumerate(self.combined):\n",
    "            tar_prob = []\n",
    "            # Iterating over all unique target values for calculating there probabilities\n",
    "            for tar in self.unique_target:\n",
    "                # calculating the probabiltiy\n",
    "                if self.feature_representation == 'bog':\n",
    "                    tar_prob.append(self.get_prob_bog(doc, tar))\n",
    "                elif self.feature_representation == 'tfidf':\n",
    "                    tar_prob.append(self.get_prob_tfidf(doc, tar))\n",
    "            tar_prob = list(map(lambda x : x / (sum(tar_prob) + 1),tar_prob)) # Normalizing\n",
    "            predicted_class = self.unique_target[tar_prob.index(max(tar_prob))]\n",
    "            if predicted_class == self.target[index]:\n",
    "                self.accuracy += 1\n",
    "        self.accuracy /= len(self.combined)\n",
    "    \n",
    "    # Score\n",
    "    def score(self):\n",
    "        if self.feature_representation == 'bog':\n",
    "            feature_rep = 'Bag of Words'\n",
    "        else:\n",
    "            feature_rep = 'TF-IDF'\n",
    "        return str(feature_rep) + ' is ' + str(np.round(self.accuracy * 100, 2)) + '%'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cc430-67dc-402c-bdc3-ffc61dc27d88",
   "metadata": {},
   "source": [
    "<h2> Using Bag of Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "421efc99-39b5-4b68-a9af-5e2d094e93b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: Bag of Words is 51.25%\n",
      "Validation Accuracy: Bag of Words is 57.0%\n",
      "Test Accuracy: Bag of Words is 52.0%\n"
     ]
    }
   ],
   "source": [
    "# Splitting the Dataset with Bag of Word Representation into Train, Validation and Test sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(movie_bog_vectors, movie_target, 0.8, 0.1)\n",
    "\n",
    "# Creating and Fitting the Naive Bayes model on the training, Validation and Test Sets\n",
    "NaiveBayes_train = NaiveBayes('bog', X_train, y_train)\n",
    "NaiveBayes_train.predict()\n",
    "NaiveBayes_val = NaiveBayes('bog', X_val, y_val)\n",
    "NaiveBayes_val.predict()\n",
    "NaiveBayes_test = NaiveBayes('bog', X_test, y_test)\n",
    "NaiveBayes_test.predict()\n",
    "\n",
    "# Calculating and Displaying the Training, Validation and Test Accuracies\n",
    "print('Training Accuracy: {}'.format(NaiveBayes_train.score()))\n",
    "print('Validation Accuracy: {}'.format(NaiveBayes_val.score()))\n",
    "print('Test Accuracy: {}'.format(NaiveBayes_test.score()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc235a-2b19-40bf-bc51-9f0aa079d8af",
   "metadata": {},
   "source": [
    "<h2> Using TF-IDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "628ac9e0-fb52-4227-b2b3-1230f990da9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: TF-IDF is 50.25%\n",
      "Validation Accuracy: TF-IDF is 56.0%\n",
      "Test Accuracy: TF-IDF is 55.0%\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(movie_tfidf_vectors, movie_target, 0.8, 0.1)\n",
    "# Creating and Fitting the Naive Bayes model on the training, Validation and Test Sets\n",
    "NaiveBayes_train = NaiveBayes('tfidf', X_train, y_train)\n",
    "NaiveBayes_train.predict()\n",
    "NaiveBayes_val = NaiveBayes('tfidf', X_val, y_val)\n",
    "NaiveBayes_val.predict()\n",
    "NaiveBayes_test = NaiveBayes('tfidf', X_test, y_test)\n",
    "NaiveBayes_test.predict()\n",
    "\n",
    "# Calculating and Displaying the Training, Validation and Test Accuracies\n",
    "print('Training Accuracy: {}'.format(NaiveBayes_train.score()))\n",
    "print('Validation Accuracy: {}'.format(NaiveBayes_val.score()))\n",
    "print('Test Accuracy: {}'.format(NaiveBayes_test.score()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387f0543-cf58-4d8a-9821-f0f544507841",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2> SVM Classifier via Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd54f9ea",
   "metadata": {},
   "source": [
    "<h3> Bag of Word Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6a352914-e0fb-49c8-bae1-2da6767c60f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for Bag of Word Representation with Grid Search:  {'C': 0.02, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "Validation Accuracy on best Hyperparameters: 66.0 2 %\n",
      "Test Accuracy on best Hyperparameters:  63.0 2 %\n"
     ]
    }
   ],
   "source": [
    "hyperparameter_grid = {'C' : [0.01,0.02,0.03], 'kernel': ['linear', 'rbf'], 'gamma': ['scale','auto']}\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(movie_bog_vectors, movie_target, 0.8, 0.1)\n",
    "#  Creating and Fitting the SVM model on the training set using Grid Search and different Hyperparameter combination\n",
    "svm = SVC(random_state=random_seed)\n",
    "#Creating a Grid Seach object with the SVM model and K-fold Cross validation\n",
    "grid_model = GridSearchCV(svm, hyperparameter_grid, n_jobs=-1, cv=5, scoring='accuracy', return_train_score=True)\n",
    "grid_model.fit(X_train, y_train)\n",
    "print('Best Hyperparameters for Bag of Word Representation with Grid Search: ', grid_model.best_params_)\n",
    "print('Validation Accuracy on best Hyperparameters:', np.round(accuracy_score(y_val, grid_model.predict(X_val)) * 100), 2, '%')\n",
    "print('Test Accuracy on best Hyperparameters: ',np.round(accuracy_score(y_test, grid_model.predict(X_test)) * 100),2, '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680acec",
   "metadata": {},
   "source": [
    "<h3> TF-IDF Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b198b5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters for TF-IDF Representation with Grid Search:  {'C': 0.01, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Validation Accuracy on best Hyperparameters:  49.0 2 %\n",
      "Test Accuracy on best Hyperparameters:  49.0 2 %\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, X_test, y_train, y_val, y_test = split_dataset(movie_tfidf_vectors, movie_target, 0.8, 0.1)\n",
    "# Creating and Fitting the SVM model on the training set using Grid Search and different Hyperparameter combination\n",
    "svm = SVC(random_state=random_seed)\n",
    "grid_model = GridSearchCV(svm, hyperparameter_grid, n_jobs=-1, cv=5, return_train_score=True)\n",
    "grid_model.fit(X_train, y_train)\n",
    "print('Best Hyperparameters for TF-IDF Representation with Grid Search: ', grid_model.best_params_)\n",
    "print('Validation Accuracy on best Hyperparameters: ', np.round(accuracy_score(y_val, grid_model.predict(X_val)) * 100),2, '%')\n",
    "print('Test Accuracy on best Hyperparameters: ', np.round(accuracy_score(y_test, grid_model.predict(X_test)) * 100),2, '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4635600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
